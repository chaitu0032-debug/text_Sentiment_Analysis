{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NIf7SkxqiH-"
      },
      "source": [
        "from keras.layers import Dropout, Dense, GRU,Input,Embedding,Flatten, MaxPooling1D, Conv1D\n",
        "\n",
        "from keras.models import Sequential\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSFBdDnPl29r"
      },
      "source": [
        "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
        "    np.random.seed(7)\n",
        "    text = np.concatenate((X_train, X_test), axis=0)\n",
        "    text = np.array(text)\n",
        "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    word_index = tokenizer.word_index\n",
        "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    indices = np.arange(text.shape[0])\n",
        "    # np.random.shuffle(indices)\n",
        "    text = text[indices]\n",
        "    print(text.shape)\n",
        "    X_train = text[0:len(X_train), ]\n",
        "    X_test = text[len(X_train):, ]\n",
        "    embeddings_index = {}\n",
        "    f = open(\"glove.6B.50d.txt\", encoding=\"utf8\")\n",
        "    for line in f:\n",
        "\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        try:\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "        except:\n",
        "            pass\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    print('Total %s word vectors.' % len(embeddings_index))\n",
        "    return (X_train, X_test, word_index,embeddings_index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5kkarQfl84t"
      },
      "source": [
        "def Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
        "    \"\"\"\n",
        "    def buildModel_RNN(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
        "    word_index in word index ,\n",
        "    embeddings_index is embeddings index, look at data_helper.py\n",
        "    nClasses is number of classes,\n",
        "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    hidden_layer = 3\n",
        "    gru_node = 32\n",
        "\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
        "                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n",
        "                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "\n",
        "\n",
        "    print(gru_node)\n",
        "    for i in range(0,hidden_layer):\n",
        "        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(GRU(gru_node, recurrent_dropout=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(nclasses, activation='softmax'))\n",
        "\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2Ys3-43VJvB"
      },
      "source": [
        "from keras.layers import Dropout, Dense,Input,Embedding,Flatten, MaxPooling1D, Conv1D\n",
        "from keras.models import Sequential,Model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers.merge import Concatenate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBbSI9Ay9t7f"
      },
      "source": [
        "def Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
        "\n",
        "    \"\"\"\n",
        "        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
        "        word_index in word index ,\n",
        "        embeddings_index is embeddings index, look at data_helper.py\n",
        "        nClasses is number of classes,\n",
        "        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
        "        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
        "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
        "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    embedding_layer = Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True)\n",
        "\n",
        "    # applying a more complex convolutional approach\n",
        "    convs = []\n",
        "    filter_sizes = []\n",
        "    layer = 5\n",
        "    print(\"Filter  \",layer)\n",
        "    for fl in range(0,layer):\n",
        "        filter_sizes.append((fl+2))\n",
        "\n",
        "    node = 128\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    for fsz in filter_sizes:\n",
        "        l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
        "        l_pool = MaxPooling1D(5)(l_conv)\n",
        "        #l_pool = Dropout(0.25)(l_pool)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "    l_merge = Concatenate(axis=1)(convs)\n",
        "    l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n",
        "    l_cov1 = Dropout(dropout)(l_cov1)\n",
        "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "    l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n",
        "    l_cov2 = Dropout(dropout)(l_cov2)\n",
        "    l_pool2 = MaxPooling1D(30)(l_cov2)\n",
        "    l_flat = Flatten()(l_pool2)\n",
        "    l_dense = Dense(1024, activation='relu')(l_flat)\n",
        "    l_dense = Dropout(dropout)(l_dense)\n",
        "    l_dense = Dense(512, activation='relu')(l_dense)\n",
        "    l_dense = Dropout(dropout)(l_dense)\n",
        "    preds = Dense(nclasses, activation='softmax')(l_dense)\n",
        "    model = Model(sequence_input, preds)\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mkM_E1rtKk1b"
      },
      "source": [
        "\n",
        "data_clean = pd.read_csv(\"train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": false,
        "_kg_hide-input": false,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ve2V7mvkKk1c",
        "outputId": "66152aa8-7df8-4835-a436-6c34f5e2b1ee"
      },
      "source": [
        "data_clean = data_clean[[\"ID\",\"text\",\"label_num\"]]\n",
        "data_clean.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>- It is not our fight - Are we not part of thi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>THAT'S THE DIFFERENCE BETWEEN YOU AND ME  YOU...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>- WHAT DO THE TITANIC AND THE SIXTH SENSE HAVE...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>\"COME ON MAN, YOU KNOW THE THING.\\r\\nJUST ASK ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>\"Those who believe without reason cannot be co...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                                               text  label_num\n",
              "0   1  - It is not our fight - Are we not part of thi...          2\n",
              "1   2   THAT'S THE DIFFERENCE BETWEEN YOU AND ME  YOU...          0\n",
              "2   3  - WHAT DO THE TITANIC AND THE SIXTH SENSE HAVE...          0\n",
              "3   4  \"COME ON MAN, YOU KNOW THE THING.\\r\\nJUST ASK ...          2\n",
              "4   5  \"Those who believe without reason cannot be co...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "A4T-DCdYKk1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41aa2462-a286-449a-8fcc-51f566551af5"
      },
      "source": [
        "\n",
        "X_train = data_clean['text']\n",
        "\n",
        "y_train = data_clean['label_num']\n",
        "\n",
        "X_train\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       - It is not our fight - Are we not part of thi...\n",
              "1        THAT'S THE DIFFERENCE BETWEEN YOU AND ME  YOU...\n",
              "2       - WHAT DO THE TITANIC AND THE SIXTH SENSE HAVE...\n",
              "3       \"COME ON MAN, YOU KNOW THE THING.\\r\\nJUST ASK ...\n",
              "4       \"Those who believe without reason cannot be co...\n",
              "                              ...                        \n",
              "1986                   Yup  still fabulous THEFRESHPICKLE\n",
              "1987               ZILLOW SAID YOUR HOME WAS WORTH WHAT?!\n",
              "1988    ZUCKERBERG GETS MONEY AND THE ANNOYING PEOPLE ...\n",
              "1989                                               #NAME?\n",
              "1990                                               #NAME?\n",
              "Name: text, Length: 1991, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff9SLYyJ5G5r",
        "outputId": "26322bea-d35d-4e31-f4d3-529e64b6d003"
      },
      "source": [
        "!pip install autocorrect\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from autocorrect import Speller\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhIp9Sss60Bd"
      },
      "source": [
        "def text_cleaner(text):\n",
        "    rules = [\n",
        "        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
        "        {r'\\s+': u' '},  # replace consecutive spaces\n",
        "        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
        "        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
        "        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
        "        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
        "        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
        "        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n",
        "        {r'^\\s+': u''}  # remove spaces at the beginning\n",
        "    ]\n",
        "    for rule in rules:\n",
        "      for (k, v) in rule.items():\n",
        "        regex = re.compile(k)\n",
        "        text = regex.sub(v, text)\n",
        "    text = text.rstrip()\n",
        "    return text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZf2UysNd39u",
        "outputId": "f85edf77-7bf1-4594-f275-4414c9591bec"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stemmer = PorterStemmer()\n",
        "words = stopwords.words(\"english\")\n",
        "# X_train_cleaned= X_train.apply(lambda x: \" \".join([(stemmer.stem(i)) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
        "X_train_cleaned=[]\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for line in X_train:\n",
        "  word_tokens = word_tokenize(line)\n",
        "\n",
        "  filtered_sentence = [w for w in word_tokens if not w in words]\n",
        "\n",
        "  filtered_sentence = \"\"\n",
        "\n",
        "  for w in word_tokens:\n",
        "    if w not in words:\n",
        "      w=ps.stem(w)\n",
        "      lemmatizer.lemmatize(w)\n",
        "      filtered_sentence=filtered_sentence+(w)+\" \"\n",
        "\n",
        "  X_train_cleaned.append(text_cleaner(filtered_sentence))\n",
        "\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       - It is not our fight - Are we not part of thi...\n",
              "1        THAT'S THE DIFFERENCE BETWEEN YOU AND ME  YOU...\n",
              "2       - WHAT DO THE TITANIC AND THE SIXTH SENSE HAVE...\n",
              "3       \"COME ON MAN, YOU KNOW THE THING.\\r\\nJUST ASK ...\n",
              "4       \"Those who believe without reason cannot be co...\n",
              "                              ...                        \n",
              "1986                   Yup  still fabulous THEFRESHPICKLE\n",
              "1987               ZILLOW SAID YOUR HOME WAS WORTH WHAT?!\n",
              "1988    ZUCKERBERG GETS MONEY AND THE ANNOYING PEOPLE ...\n",
              "1989                                               #NAME?\n",
              "1990                                               #NAME?\n",
              "Name: text, Length: 1991, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNJ0hBR55hkU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8slIQbGmnUX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X_train, y_train, test_size = 0.25, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-67sCv35m94L",
        "outputId": "8fc89329-5b7a-4d9e-ebfa-b5d02fb67c29"
      },
      "source": [
        "def tokenize(text): \n",
        "    tknzr = TweetTokenizer()\n",
        "    return tknzr.tokenize(text)\n",
        "\n",
        "en_stopwords = set(stopwords.words(\"english\")) \n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    analyzer = 'word',\n",
        "    tokenizer = tokenize,\n",
        "    lowercase = False,\n",
        "    ngram_range=(1, 1),\n",
        "    stop_words = en_stopwords)\n",
        "vectorizer.fit(X_Train)\n",
        "\n",
        "X_Train = vectorizer.transform(X_Train)\n",
        "X_Test  = vectorizer.transform(X_Test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl71f4zLmEXe",
        "outputId": "91578539-a452-4372-d1b5-73f4f1bb1a4b"
      },
      "source": [
        "\n",
        "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_Train,X_Test)\n",
        "\n",
        "\n",
        "model_CNN = Build_Model_CNN_Text(word_index,embeddings_index, 20)\n",
        "\n",
        "model_CNN.fit(X_train_Glove, Y_Train,\n",
        "                              validation_data=(X_test_Glove, Y_Test),\n",
        "                              epochs=10,\n",
        "                              batch_size=128,\n",
        "                              verbose=2)\n",
        "\n",
        "predicted = model_CNN.predict(X_test_Glove)\n",
        "\n",
        "# print(metrics.classification_report(Y_Test, predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5087 unique tokens.\n",
            "(1991, 500)\n",
            "Total 81060 word vectors.\n",
            "Filter   5\n",
            "Epoch 1/10\n",
            "12/12 - 30s - loss: 1.6209 - accuracy: 0.2974 - val_loss: 1.6904 - val_accuracy: 0.3554\n",
            "Epoch 2/10\n",
            "12/12 - 28s - loss: 1.1915 - accuracy: 0.3403 - val_loss: 1.6949 - val_accuracy: 0.3554\n",
            "Epoch 3/10\n",
            "12/12 - 28s - loss: 1.1676 - accuracy: 0.3416 - val_loss: 1.7295 - val_accuracy: 0.2972\n",
            "Epoch 4/10\n",
            "12/12 - 28s - loss: 1.1631 - accuracy: 0.3255 - val_loss: 1.6829 - val_accuracy: 0.3574\n",
            "Epoch 5/10\n",
            "12/12 - 28s - loss: 1.1277 - accuracy: 0.3382 - val_loss: 1.6395 - val_accuracy: 0.2972\n",
            "Epoch 6/10\n",
            "12/12 - 28s - loss: 1.1288 - accuracy: 0.3329 - val_loss: 1.4732 - val_accuracy: 0.3414\n",
            "Epoch 7/10\n",
            "12/12 - 28s - loss: 1.1225 - accuracy: 0.3510 - val_loss: 1.5003 - val_accuracy: 0.2972\n",
            "Epoch 8/10\n",
            "12/12 - 28s - loss: 1.1264 - accuracy: 0.3416 - val_loss: 1.4843 - val_accuracy: 0.3474\n",
            "Epoch 9/10\n",
            "12/12 - 28s - loss: 1.1209 - accuracy: 0.3376 - val_loss: 1.4956 - val_accuracy: 0.3353\n",
            "Epoch 10/10\n",
            "12/12 - 28s - loss: 1.1145 - accuracy: 0.3389 - val_loss: 1.4442 - val_accuracy: 0.3554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBkV-s54XgRj",
        "outputId": "b9ecf14d-f286-4ba9-8d82-c9db95b9f4c0"
      },
      "source": [
        "# new=Y_Test[['label_num']]\n",
        "# new['label_num']=predicted\n",
        "# f1_score(Y_Test,new\n",
        "# type(pd.Series(predicted))\n",
        "Y_Test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1292    0\n",
              "1310    1\n",
              "960     2\n",
              "107     0\n",
              "1556    2\n",
              "       ..\n",
              "1517    0\n",
              "1408    0\n",
              "1218    1\n",
              "1431    0\n",
              "597     2\n",
              "Name: label_num, Length: 498, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    }
  ]
}